\documentclass{article}

\usepackage[german]{babel}
\usepackage[latin1]{inputenc}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{color}

\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{multicol}

\usepackage[top=1cm, left=1cm, right=1cm, bottom=1cm, a4paper]{geometry}

% Mathezeug
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}


% Platzsparende Überschriften
\newcommand{\h}[1]{\vspace{1ex}\begin{center}\small\textbf{#1}\end{center}}
\newcommand{\hh}[1]{{\vspace{1pt}\hrule\vspace{1pt} \noindent\textbf{#1}}\\}
\newcommand{\hhh}[1]{{\vspace{1pt}\noindent\emph{#1:}}}
\setlength{\parindent}{2ex}

\newenvironment{tightlist}{
\begin{list}{\textbullet}{
\setlength{\topsep}{-1ex}
\setlength{\itemsep}{-1ex}
\setlength{\leftmargin}{4ex}
}
}{
\end{list}
\vspace{1ex}
}

\newcounter{Lcount}
\newenvironment{tightenum}{
\begin{list}{\arabic{Lcount}.}{
\usecounter{Lcount}
\setlength{\topsep}{-1ex}
\setlength{\itemsep}{-1ex}
\setlength{\leftmargin}{4ex}}
}{
\end{list}
\vspace{1ex}
}

% FIXME
\newcommand{\FM}[1]{{\color{red}\emph{#1}}}

\pagestyle{empty}

\begin{document}
\setlength{\topsep}{0ex}
%\setlength{\parskip}{10ex}
\setlength{\abovedisplayskip}{0ex}
\setlength{\belowdisplayskip}{0ex}

\begin{center} 1 Din A4 Zettel mit beliebigem Inhalt\end{center}
\begin{multicols}{4}
\scriptsize\raggedright

\h{Diskrete Verteilungen}
\hh{Gleichverteilung}
$p(X=x_i)=f_X(x_i)=\frac{1}{n}\ (i=1,\ldots,n)$
$EX = \frac{1}{n} \sum_{i=1}^n x_i$

\hh{Binomialverteilung} $X \sim B(n,p)$
$p_X(k)={n \choose k}p^k(1-p)^{n-k}$
$F_X(x)=\displaystyle\sum_{k\le x}{n \choose k}p^k(1-p)^{n-k}$
$EX = np, \var(X) = np(1-p)$ \\
$g_X(s) = (1 - p + e^{s}p)^n $

\hh{Hypergeometrische Verteilung}
$X \sim\text{Hypergeom}(n,r,m)$
$p_X(k)=\displaystyle\frac{{r \choose k}{n-r \choose m-k}}{{n \choose m}}$ \\
$EX = \frac{rm}{n}$ \\
$\var(X) = \frac{rm}{n}(1-\frac{m}{n})(\frac{n-r}{n-1})$

\hh{Geometrische Verteilung}
W'keit beim $k$-ten Versuch den 1. Treffer zu bekommen, $p$ Trefferw'keit.
$p_X(k)=(1-p)^{k-1}p$
$EX=\frac{1}{p}, \, \var(X)=\frac{1-p}{p^2}$

\hh{Poisson-Verteilung}
$p_X(k)=e^{-\lambda}\frac{\lambda^k}{k!}\ (k=0, \ldots)$
$EX = \var(X) = \lambda$

\h{Stetige Verteilungen}
\hh{Gleichverteilung}
$X \sim U(a,b)$
$f_X(x)=\begin{cases}\frac{1}{b-a}, &a<x<b\\ 0,&\text{sonst}\end{cases}$
$F_X(x)=\frac{x-a}{b-a}$ auf $(a,b)$
$EX = \frac{a+b}{2}, \, \var(X) = \frac{1}{12}(b-a)^2$

\hh{Exponentialverteilung}
$X\sim$ Exp$(\lambda), \lambda>0$
$f_X(x)=\begin{cases}\lambda e^{-\lambda x},&x\ge 0 \\ 0,&x<0\end{cases}$
$F_X(x)=1-e^{-\lambda x}$
$EX = \frac{1}{\lambda}, \, \var(X) = \frac{1}{\lambda ^2}$
\hhh{Gedächtnislosigkeit} $P(X\ge t | X\ge s)=P(X\ge t-s)\ (0<s<t)$

\hh{Normalverteilung}
$X\sim N(\mu,\sigma^2)$
$f_X(x)=\displaystyle\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$
\hhh{Standardnormalverteilung} $\mu = 0, \, \sigma ^2 = 1$\\
$\Phi(x)=\displaystyle\int_{-\infty}^x\displaystyle\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{y^2}{2}\right)\text{d}y$
$\Phi(x)=1-\Phi(-x)$
$EX = \mu, \, \var(X) = \sigma ^2$\\
Sei $X\sim N(\mu,\sigma^2)$.
$\Rightarrow Z:=\frac{X-\mu}{\sigma} \, \sim N(0,1)$\\
\hhh{Faltungsstabil} $X_1+X_2 \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$
$X\sim N(0,1):$\\
$\varphi_X(t) = \int_{-\infty}^{\infty} \cos(tx) \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} dx$ \\
$\varphi_X'(t) = -t\varphi_X(t)$

\hh{Gammaverteilung}
$X \sim G(\alpha, \lambda)$
$f_X(x)=\begin{cases} \frac{\lambda^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x} ,&x\ge 0 \\ 0,&x<0\end{cases}$,
wobei $\Gamma(\alpha) := \int_{o}^{\infty}t^{\alpha-1}e^{-t}\text{d}t$
$EX = \frac{\alpha}{\lambda}, \, \var(X) = \frac{\alpha}{\lambda^2}$
$X \sim G(\alpha, \lambda_1), Y \sim G(\alpha, \lambda_2) \Rightarrow X+Y \sim G(\alpha, \lambda_1+ \lambda_2)$ \\
f"ur $\alpha = 1$ erh"alt man die Exponentialverteilung. 


\h{Formeln}
\hh{Siebformel}
\vspace{-1em}
\begin{multline*}
P(\bigcup_{k=1}^{n}A_k)= \sum_{k=1}^{n}(-1)^{k-1}\\\cdot\sum_{\mathclap{1\leq i_1 < i_2 \cdots < i_k\leq n}} P(A_{i1}\cap \cdots \cap A_{ik})
\end{multline*}

\hh{Erwartungswert}
(Existenz falls mit $|\cdot|$ noch $<\infty$)\\
\hhh{diskret}
$EX = \sum_{i=1}^m x_i P(A_i)$\\
\hhh{stetig}
$EX= \int_{-\infty}^{\infty} xf(x)dx$\\
$E(aX+bY) = aEX+bEY$\\
Falls $X_i$ unkorreliert:
$E(\prod_{i=1}^n X_i) = \prod_{i=1}^nE(X_i)$
\hhh{Erwartungsvektor}
$X=(X_1,\dots,X_n)$ ZV, $EX_i^2 < \infty: EX:=(EX_1,\dots,EX_n)$
\hh{Varianz,Stdabw}
$\var(X) := E((X-EX)^2) = EX^2 - (EX)^2 $\\
$\var(aX+b) = a^2\var(X)$\\
Falls $X_i$ unkorreliert:
$\var(\sum_{i=1}^n X_i) = \sum_{i=1}^n\var(X_i)$
\hhh{Standardabweichung}
$\sigma(X) := \sqrt{\var(X)}$\\

\hh{Urnenmodell}
Mit Zur"uckl., mit Reihenflg.:
$n^k$\\
Mit Zur"uckl., ohne Reihenflg.:
$n+k-1 \choose k$\\
Ohne Zur"uckl., mit Reihenflg.:
$\frac{n!}{(n-k)!}$\\
Ohne Zur"uckl., ohne Reihenflg.:
$n \choose k$

\hh{Bedinge Wahrscheinlichkeit}
$P(A|B)=\frac{P(A\cap B)}{P(B)}$

\hh{Formel von Bayes}
$P(B_k|A)=\frac{P(B_k)\cdot P(A|B_k)}{\sum_{j=1}^\infty P(B_j)\cdot P(A|B_j)}$

\hh{Tschebyscheff Ungleichung}
$P(|X-EX|\geq \varepsilon )\leq \frac{1}{\varepsilon^2} \var(X)$

\hh{Faltungsformel}
$(X,Y)$ abs stetige ZV, mit gem Dichte $f_{XY} \Rightarrow Z:=X+Y$ ist abs stetige ZV mit Dichte: $f_Z(x) = \int_{-\infty}^{\infty} f_{X,Y}(t,x-t)dt$\\
$X,Y$ unabh $\Rightarrow$ Faltungsformel: $f_Z(x) = \int_{-\infty}^{\infty} f_X(t)f_Y(x-t)dt$

\hh{Unabh"angingkeit von ZV}
$X,Y$ unabh. $\Rightarrow EXY = EXEY$\\
$X_1,\dots,X_n$ unabh $\Rightarrow \var(X_1+\dots+X_n) = \sum_{i=1}^n \var(X_i)$

\hh{Ungl von Cauchy-Schwarz}
$X,Y$ ZV, $\var(X),\var(Y)$ existieren $\Rightarrow (EXY)^2 \leq EX^2EY^2$

\hh{Kovarianz}
$\cov(X,Y) = E(X-EX)(Y-EY) = EXY - EXEY$\\
$\cov(X,X) = \var(X)$\\
\hhh{$X,Y$ unkorreliert} gdw. $\cov(X,Y) = 0$\\
$X,Y$ unabh. $\Rightarrow X,Y$ unkorr.\\
\hhh{Kovarianzmatrix}
$X=(X_1,\dots,X_n) \Rightarrow \cov(X):=(\cov(X_i,X_j))_{i,j=1,\dots,n}$ \\
\hhh{Korrelationskoeffizient}
Ist $\var(X)\cdot\var(Y)>0$ so gilt:\\
$\left|\rho(X,Y) := \frac{\cov(X,Y)}{\sqrt{\var(X)\var(Y)}}\right|\le 1$

\hh{Erzeugende Fkt (diskret)}
$g_X:[-1,1] \rightarrow R$\\
$g_X(s) = \sum_{k=0}^{\infty} p_X(k)s^k = Es^X$\\
$g_X(1) = 1, p_X(k) = \frac{g_X^{(k)}(0)}{k!}$\\
$EX = g_X'(1-), \var(X) = g_X''(1-)+g_X'(1-) - (g_X'(1-))^2$\\
$X,Y$ unabh,diskret $\Rightarrow g_{X+Y}(s)=g_X(s)g_Y(s)$

\h{Konvergenzbegriff f"ur ZV}
\hh{P-fast sicher}
$X_n\stackrel{fs}{\rightarrow}X$ wenn:
\[P(\{\omega\in\Omega | \lim_{\mathclap{n\to\infty}} X_n(\omega)=X(\omega)\})=1\]

\hh{in Wahrs'keit, stochastisch}
$X_n\stackrel{P}{\rightarrow }X$ wenn, $\forall\varepsilon > 0$
\[\lim_{\mathclap{n\to\infty}}P(\{\omega\in\Omega \Bigr| |X_n(\omega)-X(\omega)|\geq \varepsilon \})=0\]

\hh{in Verteilung}
$X_n\stackrel{d}{\rightarrow }X$ wenn $\forall x$ mit $F_X$ stetig:
\[\lim_{\mathclap{n\to\infty}} F_{X_n}(x)=F_X(x)\]

\hh{Implikationen}
f.s. $\Rightarrow$ in Wahrs'keit\\
in Wahrs'keit $\Rightarrow$ in Verteilung\\
in Verteilung gegen eine Konstante $c \in R \Rightarrow$ in Wahrs'keit

\h{Char Funktionen}
$X$ ZV, $\varphi_X : \mathbb{R} \rightarrow \mathbb{C}$
\[
\varphi_X(t) := Ee^{itX} = E\cos(tX)+iE\sin(tX)
\]
$X$ diskret $\Rightarrow \varphi_X(t)=g_X(e^{it})$\\
$X$ abs stetig $\Rightarrow \varphi_X(t) = \int_{-\infty}^\infty e^{itx} f_X(x) dx$\\
$\varphi_X(0)=1$
$|\varphi_X(t)|\leq 1\quad \forall\, t\in \mathbb{R}$
$a,b\in \mathbb{R}$: $\qquad\varphi_{aX+b}(t)=e^{ibt}\varphi_X(at)$
$\varphi_X(t)$ ist glm stetig auf $\mathbb{R}$\\
$X,Y$ unabh ZV $\Rightarrow \varphi_{X+Y}(t) = \varphi_X(t) \varphi_Y(t)$\\
$E|X|^n<\infty,\,n\in N \Rightarrow \varphi_X n$-mal db und: $\varphi_X^{(n)}(0)=i^nEX^n$\\
$X,Y$ ZV mit gleicher char Fkt, so auch gleiche Verteilung.\\
$(X_n)$ Folge von ZV mit $F_{X_n}(x)$ und $\varphi_{X_n}(t)$ so ist "aquivalent:\\
$X_n\stackrel{d}{\rightarrow}X$\\
$\varphi_{X_n}(t)\rightarrow \varphi(t)\qquad \forall\, t\in R$ und $\varphi$ stetig in 0

\h{Testtheorie}
Überprüfen, ob ein Parameter $\theta$ einer Verteilung in $\Theta_0$ oder $\Theta_1$ ($\Theta_0+\Theta_1=\Theta$) ist.\\
\hhh{Einseitiger Test}\\ $H_0: \theta \le \theta_0$ vs. $H_1: \theta > \theta_0$\\
\hhh{Zweiseitiger Test}\\ $H_0: \theta = \theta_0$ vs. $H_1: \theta \ne \theta_0$
\hh{Fehler und Güte}
\hhh{1. Art} Test: "`$H_1$"' aber $H_0$ wahr.\\
\hhh{2. Art} Test: "`$H_0$"' aber $H_1$ wahr.\\
\hhh{Güte} $\alpha$ := W'keit für Fehler 1. Art.
Man legt $\alpha$ fest. Also muss der Fehler 1. Art der schlimmere sein (z.B. unwirksames Medikament als wirksam bedacht oder Ham als Spam eingeordnet). Danach erst minimiert man den Fehler 2. Art. Eselsbrücke: \textbf{H}ypothese $H_0$ $\hat=$ \textbf{H}am.

\end{multicols}
\end{document}
